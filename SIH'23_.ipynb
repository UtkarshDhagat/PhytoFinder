{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5ClS7o9rIL0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "directory_path = '/content/drive/My Drive/final'\n",
        "excel_files = [f for f in os.listdir(directory_path) if f.endswith('.xlsx') or f.endswith('.xls')]\n",
        "\n",
        "dataframes = {}\n",
        "\n",
        "for file in excel_files:\n",
        "    file_path = os.path.join(directory_path, file)\n",
        "    df = pd.read_excel(file_path)\n",
        "    dataframes[file] = df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdoonaqlrIT9",
        "outputId": "28091fd8-04ad-41d4-f5df-876c81724a5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "60/60 [==============================] - 959s 16s/step - loss: 4.0825 - accuracy: 0.0818 - val_loss: 3.5641 - val_accuracy: 0.2383\n",
            "Epoch 2/10\n",
            "60/60 [==============================] - 885s 15s/step - loss: 3.2472 - accuracy: 0.2549 - val_loss: 2.9115 - val_accuracy: 0.3557\n",
            "Epoch 3/10\n",
            "60/60 [==============================] - 877s 15s/step - loss: 2.5270 - accuracy: 0.4111 - val_loss: 2.4656 - val_accuracy: 0.4270\n",
            "Epoch 4/10\n",
            "60/60 [==============================] - 874s 15s/step - loss: 1.9879 - accuracy: 0.5186 - val_loss: 2.1753 - val_accuracy: 0.4710\n",
            "Epoch 5/10\n",
            "60/60 [==============================] - 877s 15s/step - loss: 1.5878 - accuracy: 0.6303 - val_loss: 1.9210 - val_accuracy: 0.5213\n",
            "Epoch 6/10\n",
            "60/60 [==============================] - 873s 15s/step - loss: 1.2721 - accuracy: 0.7074 - val_loss: 1.7833 - val_accuracy: 0.5437\n",
            "Epoch 7/10\n",
            "60/60 [==============================] - 872s 15s/step - loss: 1.0299 - accuracy: 0.7609 - val_loss: 1.6570 - val_accuracy: 0.5737\n",
            "Epoch 8/10\n",
            "60/60 [==============================] - 873s 15s/step - loss: 0.8363 - accuracy: 0.8154 - val_loss: 1.5415 - val_accuracy: 0.6087\n",
            "Epoch 9/10\n",
            "60/60 [==============================] - 873s 15s/step - loss: 0.6718 - accuracy: 0.8642 - val_loss: 1.4967 - val_accuracy: 0.6087\n",
            "Epoch 10/10\n",
            "60/60 [==============================] - 851s 14s/step - loss: 0.5620 - accuracy: 0.8888 - val_loss: 1.4588 - val_accuracy: 0.6191\n",
            "45/45 [==============================] - 374s 8s/step - loss: 1.5083 - accuracy: 0.6143\n",
            "\n",
            "Test accuracy: 0.6142557859420776\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "# Path to your data\n",
        "base_dir = '/content/drive/My Drive/final'\n",
        "\n",
        "# Get a list of all images and their class names\n",
        "data = []\n",
        "for root, dirs, files in os.walk(base_dir):\n",
        "    for file in files:\n",
        "        if file.endswith('.jpg'):  # adjust this as necessary\n",
        "            data.append((os.path.join(root, file), os.path.basename(root)))\n",
        "\n",
        "# Convert to a DataFrame\n",
        "data = pd.DataFrame(data, columns=['filename', 'class'])\n",
        "\n",
        "# Split the data\n",
        "train_data, temp_data = train_test_split(data, test_size=0.6, stratify=data['class'])\n",
        "val_data, test_data = train_test_split(temp_data, test_size=0.5, stratify=temp_data['class'])\n",
        "\n",
        "# Preprocessing functions\n",
        "def preprocess_image(image):\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, [150, 150])\n",
        "    image /= 255.0  # normalize to [0,1] range\n",
        "    return image\n",
        "\n",
        "def load_and_preprocess_image(path):\n",
        "    image = tf.io.read_file(path)\n",
        "    return preprocess_image(image)\n",
        "\n",
        "def load_and_preprocess_from_path_label(path, label):\n",
        "    return load_and_preprocess_image(path), label\n",
        "\n",
        "# Create a label index\n",
        "label_names = train_data['class'].unique().tolist()\n",
        "label_to_index = dict((name, index) for index, name in enumerate(label_names))\n",
        "index_to_label = dict((index, name) for index, name in enumerate(label_names))\n",
        "\n",
        "# Convert class names to class indices\n",
        "train_data['class'] = train_data['class'].map(label_to_index)\n",
        "val_data['class'] = val_data['class'].map(label_to_index)\n",
        "test_data['class'] = test_data['class'].map(label_to_index)\n",
        "\n",
        "# Create datasets\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_data['filename'], train_data['class'])).map(load_and_preprocess_from_path_label)\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_data['filename'], val_data['class'])).map(load_and_preprocess_from_path_label)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((test_data['filename'], test_data['class'])).map(load_and_preprocess_from_path_label)\n",
        "\n",
        "# Batch and prefetch\n",
        "train_ds = train_ds.batch(32).prefetch(1)\n",
        "val_ds = val_ds.batch(32).prefetch(1)\n",
        "test_ds = test_ds.batch(32).prefetch(1)\n",
        "\n",
        "# Load the VGG16 model but exclude the top layers\n",
        "base_model = VGG16(include_top=False, weights='imagenet', input_shape=(150, 150, 3))\n",
        "\n",
        "# Freeze the base_model\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create a new model on top\n",
        "model = tf.keras.models.Sequential([\n",
        "    base_model,\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(1024, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(len(label_names), activation='softmax')\n",
        "])\n",
        "\n",
        "# You might want to fine-tune some layers\n",
        "for layer in base_model.layers[-4:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Use a smaller learning rate\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=10,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(test_ds, verbose=1)\n",
        "print('\\nTest accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wt93F9H6rIam"
      },
      "outputs": [],
      "source": [
        "img_path = '/content/drive/My Drive/Segmented Medicinal Leaf Images 2/carisa.webp'  # replace with your actual path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e36dd0HrIdn"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load the image for displaying\n",
        "img_display = image.load_img(img_path)\n",
        "\n",
        "# Load the image for prediction and resize\n",
        "img = image.load_img(img_path, target_size=(64, 64))  # Adjust target_size to match your model's input shape\n",
        "img_array = image.img_to_array(img)\n",
        "img_array_expanded = np.expand_dims(img_array, axis=0)\n",
        "img_array_expanded /= 255.  # If your model's training input was normalized, do the same for prediction\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(img_display)\n",
        "plt.show()\n",
        "\n",
        "# Make a prediction\n",
        "prediction = model.predict(img_array_expanded)\n",
        "predicted_class = np.argmax(prediction[0], -1)\n",
        "\n",
        "# Get the class names from the generator\n",
        "class_names = list(train_generator.class_indices.keys())\n",
        "plant_name = class_names[predicted_class]\n",
        "\n",
        "print(\"The predicted plant is:\", plant_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQVXqC1arIfo"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pylab as plt\n",
        "\n",
        "# Extract accuracy and loss from history object\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(len(acc))\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.ylabel(\"Accuracy (training and validation)\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.ylabel(\"Loss (training and validation)\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KY40dKmdrIhz"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model.save('my_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUFFr3DT6BgI"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test data\n",
        "test_loss, test_acc = model.evaluate(test_generator, verbose=1)\n",
        "\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}